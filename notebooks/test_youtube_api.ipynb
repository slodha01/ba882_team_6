{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "1e001214",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from googleapiclient.discovery import build\n",
    "from googleapiclient.errors import HttpError\n",
    "from datetime import datetime\n",
    "\n",
    "# Global variable to cache the YouTube client\n",
    "_youtube_client = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d44ce2a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_youtube_client():\n",
    "    \"\"\"\n",
    "    Lazy initialization of YouTube API client.\n",
    "    Only creates client when first called, not at import time.\n",
    "    \"\"\"\n",
    "    global _youtube_client\n",
    "    \n",
    "    if _youtube_client is None:\n",
    "        API_KEY = os.environ.get('YOUTUBE_API_KEY')\n",
    "        if not API_KEY:\n",
    "            raise ValueError(\"YOUTUBE_API_KEY environment variable not set!\")\n",
    "        \n",
    "        _youtube_client = build('youtube', 'v3', developerKey=API_KEY)\n",
    "        print(\"YouTube API client initialized\")\n",
    "    \n",
    "    return _youtube_client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5d1643a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load_dotenv()\n",
    "# API_KEY = os.getenv(\"YOUTUBE_API_KEY\")\n",
    "\n",
    "# if not API_KEY:\n",
    "#     raise ValueError(\"YouTube API key not found in .env file!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00a3405b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# BASE_URL = \"https://www.googleapis.com/youtube/v3\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac1ff225",
   "metadata": {},
   "outputs": [],
   "source": [
    "# youtube = build('youtube', 'v3', developerKey=API_KEY)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cce7aec",
   "metadata": {},
   "source": [
    "The \"order\" parameter defines the sort order of the returned videos.\n",
    "\n",
    "| order        | What it means                                                                   |\n",
    "| ------------ | ------------------------------------------------------------------------------- |\n",
    "| `date`       | Sorts videos by **publish date**, newest first (default if you set `channelId`) |\n",
    "| `rating`     | Sorts by highest viewer rating                                                  |\n",
    "| `relevance`  | Sorts by relevance to the search query (default if you use `q=` search term)    |\n",
    "| `title`      | Sorts alphabetically by title                                                   |\n",
    "| `videoCount` | Sorts channels by number of uploaded videos                                     |\n",
    "| `viewCount`  | Sorts by number of views (most viewed first)                                    |\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "31685439",
   "metadata": {},
   "outputs": [],
   "source": [
    "# If \"order\" is not used, instead we are using \"q\", it returns in order of relevance by default (\"relevance\" is the default option)\n",
    "\n",
    "def get_video(query, max_results=50, order='relevance'):\n",
    "    \"\"\"\n",
    "    Search for videos by keyword.\n",
    "    Returns DataFrame with video metadata.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        youtube = get_youtube_client()\n",
    "        response = youtube.search().list(\n",
    "            q=query,\n",
    "            part='id,snippet',\n",
    "            maxResults=min(max_results, 50),\n",
    "            type='video',\n",
    "            order='date'\n",
    "        ).execute()\n",
    "\n",
    "        videos = []\n",
    "        for item in response.get('items', []):\n",
    "            snippet = item['snippet']\n",
    "            videos.append({\n",
    "                'video_id': item['id']['videoId'],\n",
    "                'channel_id': snippet['channelId'],\n",
    "                'title': snippet['title'],\n",
    "                'description': snippet['description'],\n",
    "                'published_at': snippet['publishedAt'],\n",
    "                'search_query': query,\n",
    "                'search_order': order\n",
    "            })\n",
    "        \n",
    "        return pd.DataFrame(videos)\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"Error in get_video: {str(e)}\")\n",
    "        return pd.DataFrame()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "3d3758bc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>video_id</th>\n",
       "      <th>channel_id</th>\n",
       "      <th>title</th>\n",
       "      <th>description</th>\n",
       "      <th>published_at</th>\n",
       "      <th>search_query</th>\n",
       "      <th>search_order</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>F6lad-lTI8A</td>\n",
       "      <td>UCFsGwtv75tC3f_GIKQQ5sTA</td>\n",
       "      <td>Free Databricks Data Engineer Associate Origio...</td>\n",
       "      <td>shorts Course Link: https://www.udemy.com/cour...</td>\n",
       "      <td>2025-10-13T15:00:25Z</td>\n",
       "      <td>Data Engineering</td>\n",
       "      <td>relevance</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0aepiWWFutw</td>\n",
       "      <td>UCUTDd3ieNmKDtHu4VJAsNZA</td>\n",
       "      <td>Top 5 File Format for Data Engineering | Json ...</td>\n",
       "      <td></td>\n",
       "      <td>2025-10-13T14:26:03Z</td>\n",
       "      <td>Data Engineering</td>\n",
       "      <td>relevance</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>rLJP85qE8J4</td>\n",
       "      <td>UCoUjs_Z9JhDEi6g0Tba2wJA</td>\n",
       "      <td>Data Engineer ‡§ï‡§∏‡§Ç ‡§¨‡§®‡§æ‡§≤? How to become a Data E...</td>\n",
       "      <td>Data Engineer ‡§ï‡§∏‡§Ç ‡§¨‡§®‡§æ‡§≤? #dataengineering #data...</td>\n",
       "      <td>2025-10-13T13:24:52Z</td>\n",
       "      <td>Data Engineering</td>\n",
       "      <td>relevance</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>CX0fn0gbb4k</td>\n",
       "      <td>UCucTCSBBt5pCzoBt4P9XsKw</td>\n",
       "      <td>Research groups Embedded Communicating Systems...</td>\n",
       "      <td>Discover how data engineering, automated syste...</td>\n",
       "      <td>2025-10-13T12:55:45Z</td>\n",
       "      <td>Data Engineering</td>\n",
       "      <td>relevance</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>NU2Cu9JHYKM</td>\n",
       "      <td>UCYJhto4Of0p8eKKxmB2un9g</td>\n",
       "      <td>SHOCKING Truth About Landing A Job At Zeta, Ba...</td>\n",
       "      <td>Top MNCs Zeta, Baker Hughes, SAP, and Microsof...</td>\n",
       "      <td>2025-10-13T12:40:00Z</td>\n",
       "      <td>Data Engineering</td>\n",
       "      <td>relevance</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      video_id                channel_id  \\\n",
       "0  F6lad-lTI8A  UCFsGwtv75tC3f_GIKQQ5sTA   \n",
       "1  0aepiWWFutw  UCUTDd3ieNmKDtHu4VJAsNZA   \n",
       "2  rLJP85qE8J4  UCoUjs_Z9JhDEi6g0Tba2wJA   \n",
       "3  CX0fn0gbb4k  UCucTCSBBt5pCzoBt4P9XsKw   \n",
       "4  NU2Cu9JHYKM  UCYJhto4Of0p8eKKxmB2un9g   \n",
       "\n",
       "                                               title  \\\n",
       "0  Free Databricks Data Engineer Associate Origio...   \n",
       "1  Top 5 File Format for Data Engineering | Json ...   \n",
       "2  Data Engineer ‡§ï‡§∏‡§Ç ‡§¨‡§®‡§æ‡§≤? How to become a Data E...   \n",
       "3  Research groups Embedded Communicating Systems...   \n",
       "4  SHOCKING Truth About Landing A Job At Zeta, Ba...   \n",
       "\n",
       "                                         description          published_at  \\\n",
       "0  shorts Course Link: https://www.udemy.com/cour...  2025-10-13T15:00:25Z   \n",
       "1                                                     2025-10-13T14:26:03Z   \n",
       "2  Data Engineer ‡§ï‡§∏‡§Ç ‡§¨‡§®‡§æ‡§≤? #dataengineering #data...  2025-10-13T13:24:52Z   \n",
       "3  Discover how data engineering, automated syste...  2025-10-13T12:55:45Z   \n",
       "4  Top MNCs Zeta, Baker Hughes, SAP, and Microsof...  2025-10-13T12:40:00Z   \n",
       "\n",
       "       search_query search_order  \n",
       "0  Data Engineering    relevance  \n",
       "1  Data Engineering    relevance  \n",
       "2  Data Engineering    relevance  \n",
       "3  Data Engineering    relevance  \n",
       "4  Data Engineering    relevance  "
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "videos_df = get_video('Data Engineering', max_results=20)\n",
    "videos_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "6b6ded48",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Retrieve video details including statistics from video IDs (most relevant videos by default from the function above)\n",
    "\n",
    "def get_channel_details(channel_ids):\n",
    "    \"\"\"\n",
    "    Retrieve basic channel information for a list of channel IDs.\n",
    "    Returns DataFrame with channel metadata.\n",
    "    \"\"\"\n",
    "    if not channel_ids:\n",
    "        return pd.DataFrame()\n",
    "    \n",
    "    try:\n",
    "        youtube = get_youtube_client()\n",
    "        # API accepts max 50 IDs per request\n",
    "        all_channels = []\n",
    "        for i in range(0, len(channel_ids), 50):\n",
    "            batch = channel_ids[i:i+50]\n",
    "            response = youtube.channels().list(\n",
    "                part=\"snippet,statistics\",\n",
    "                id=\",\".join(batch)\n",
    "            ).execute()\n",
    "\n",
    "            for item in response.get('items', []):\n",
    "                snippet = item['snippet']\n",
    "                stats = item['statistics']\n",
    "                all_channels.append({\n",
    "                    'channel_id': item['id'],\n",
    "                    'channel_title': snippet['title'],\n",
    "                    'description': snippet.get('description'),\n",
    "                    'country': snippet.get('country'),\n",
    "                    'published_at': snippet['publishedAt'],\n",
    "                    'subscriber_count': int(stats.get('subscriberCount', 0)),\n",
    "                    'video_count': int(stats.get('videoCount', 0)),\n",
    "                    'view_count': int(stats.get('viewCount', 0))\n",
    "                })\n",
    "        \n",
    "        return pd.DataFrame(all_channels)\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"Error in get_channel_details: {str(e)}\")\n",
    "        return pd.DataFrame()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "073956da",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>channel_id</th>\n",
       "      <th>channel_title</th>\n",
       "      <th>description</th>\n",
       "      <th>country</th>\n",
       "      <th>published_at</th>\n",
       "      <th>subscriber_count</th>\n",
       "      <th>video_count</th>\n",
       "      <th>view_count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>UCoUjs_Z9JhDEi6g0Tba2wJA</td>\n",
       "      <td>Tech Sagar Bhujang</td>\n",
       "      <td>Hey Guys,\\n\\nWelcome to my YouTube channel!!!\\...</td>\n",
       "      <td>IN</td>\n",
       "      <td>2020-05-17T14:44:16.169665Z</td>\n",
       "      <td>17</td>\n",
       "      <td>13</td>\n",
       "      <td>4461</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>UCng7Xx4xhx6pyD1pj1aZQpg</td>\n",
       "      <td>SFDCGYM</td>\n",
       "      <td>SFDCGYM‚Äôs Live Class Training Program helps te...</td>\n",
       "      <td>IN</td>\n",
       "      <td>2021-08-20T09:56:47.760813Z</td>\n",
       "      <td>1140</td>\n",
       "      <td>33</td>\n",
       "      <td>63814</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>UCaTdPhA5f8NC0SZ8uSzsiEw</td>\n",
       "      <td>Pule‚Äôs diary</td>\n",
       "      <td>Hey, I‚Äôm Pule, a data engineer by profession. ...</td>\n",
       "      <td>None</td>\n",
       "      <td>2020-04-29T11:21:42.306197Z</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>232</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>UCW21hcjn5O7_MLNxIy8kH8g</td>\n",
       "      <td>Tech Career Hubs</td>\n",
       "      <td>Hi everyone!\\nMy name is Venkata Sri Hari, and...</td>\n",
       "      <td>IN</td>\n",
       "      <td>2025-04-02T06:05:11.428129Z</td>\n",
       "      <td>75</td>\n",
       "      <td>14</td>\n",
       "      <td>1922</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>UCIPDZxzZn-c0_B7OHphL3QQ</td>\n",
       "      <td>CodeQueryHub</td>\n",
       "      <td>Welcome to CodeQueryHub ‚Äì your hub for masteri...</td>\n",
       "      <td>IN</td>\n",
       "      <td>2025-09-26T05:13:27.804913Z</td>\n",
       "      <td>79</td>\n",
       "      <td>34</td>\n",
       "      <td>2649</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>UCWmwh8DhJstPi34JzUZFyBw</td>\n",
       "      <td>DataPopkorn</td>\n",
       "      <td>DataPopkorn.com offers quick, expert-led data ...</td>\n",
       "      <td>GB</td>\n",
       "      <td>2024-07-22T22:20:41.883101Z</td>\n",
       "      <td>303</td>\n",
       "      <td>176</td>\n",
       "      <td>5895</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>UCYJhto4Of0p8eKKxmB2un9g</td>\n",
       "      <td>FrontLinesMedia</td>\n",
       "      <td>Frontlines Media is a new generation Edu Tech ...</td>\n",
       "      <td>IN</td>\n",
       "      <td>2019-08-05T13:01:42Z</td>\n",
       "      <td>450000</td>\n",
       "      <td>2222</td>\n",
       "      <td>61917694</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>UC_n9wCmDG064tZUKZF2g4Aw</td>\n",
       "      <td>WafaStudies</td>\n",
       "      <td>In this Channel, you can find Videos and Playl...</td>\n",
       "      <td>IN</td>\n",
       "      <td>2012-06-02T06:54:56Z</td>\n",
       "      <td>108000</td>\n",
       "      <td>561</td>\n",
       "      <td>16973403</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>UCdzWNo6L2TELV-3Gb9dBMWQ</td>\n",
       "      <td>Rasoel Barakhoev</td>\n",
       "      <td>My mission is to help you launch a high-paying...</td>\n",
       "      <td>NL</td>\n",
       "      <td>2017-11-02T13:51:20Z</td>\n",
       "      <td>6</td>\n",
       "      <td>12</td>\n",
       "      <td>1458</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>UC5e3U5r0gCO_--vi2g-vDvg</td>\n",
       "      <td>Worleybird Innovation</td>\n",
       "      <td>Worleybird Innovation - Where AI Meets Actual ...</td>\n",
       "      <td>US</td>\n",
       "      <td>2025-07-23T19:04:10.526848Z</td>\n",
       "      <td>27</td>\n",
       "      <td>106</td>\n",
       "      <td>20673</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>UC4-iDkADam__NA5ZPh7YrpQ</td>\n",
       "      <td>Interview Kickstart India</td>\n",
       "      <td>Welcome to Interview Kickstart. \\nThe India's ...</td>\n",
       "      <td>None</td>\n",
       "      <td>2025-07-17T13:30:11.677909Z</td>\n",
       "      <td>9440</td>\n",
       "      <td>83</td>\n",
       "      <td>85154</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>UCl8BC-R6fqITW9UrSXj5Uxg</td>\n",
       "      <td>Azarudeen Shahul</td>\n",
       "      <td>Tutorial on Apache Spark (PySpark), Spark with...</td>\n",
       "      <td>IN</td>\n",
       "      <td>2012-03-31T15:41:39Z</td>\n",
       "      <td>14500</td>\n",
       "      <td>126</td>\n",
       "      <td>1319831</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>UCjKtoq0I2vRqk7HFgb07U3g</td>\n",
       "      <td>Culture Data &amp; IA</td>\n",
       "      <td>Abonne-toi si tu es int√©ress√©(e) par la Data. ...</td>\n",
       "      <td>None</td>\n",
       "      <td>2025-03-21T06:36:52.099792Z</td>\n",
       "      <td>642</td>\n",
       "      <td>21</td>\n",
       "      <td>2009</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>UCucTCSBBt5pCzoBt4P9XsKw</td>\n",
       "      <td>School of Engineering</td>\n",
       "      <td>Au coeur du nouveau Campus Energypolis de Sion...</td>\n",
       "      <td>CH</td>\n",
       "      <td>2022-10-24T06:33:05.502043Z</td>\n",
       "      <td>184</td>\n",
       "      <td>119</td>\n",
       "      <td>40314</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>UCbjozK_PYCTLEluFlrJ8UZg</td>\n",
       "      <td>Durga Software Solutions</td>\n",
       "      <td>DURGA Software Solutions is an Institute, whic...</td>\n",
       "      <td>IN</td>\n",
       "      <td>2014-02-03T04:15:47Z</td>\n",
       "      <td>850000</td>\n",
       "      <td>54584</td>\n",
       "      <td>188227461</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>UCkC559DvvBCiLUCH_qHNpCA</td>\n",
       "      <td>Data Engineering with Subhadip</td>\n",
       "      <td>This channel is for demonstrating the data pro...</td>\n",
       "      <td>CA</td>\n",
       "      <td>2017-12-09T12:05:51Z</td>\n",
       "      <td>35</td>\n",
       "      <td>120</td>\n",
       "      <td>9769</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>UCEmQS-xEmfJ2GY-ckesPa9Q</td>\n",
       "      <td>Geospatial Solution</td>\n",
       "      <td></td>\n",
       "      <td>KE</td>\n",
       "      <td>2021-11-17T11:58:50.040058Z</td>\n",
       "      <td>5160</td>\n",
       "      <td>92</td>\n",
       "      <td>224446</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>UCUTDd3ieNmKDtHu4VJAsNZA</td>\n",
       "      <td>Data With Pranjal</td>\n",
       "      <td>Welcome to \"Data with Pranjal\"\\nI'm Pranjal, a...</td>\n",
       "      <td>IN</td>\n",
       "      <td>2024-06-14T17:32:49.45086Z</td>\n",
       "      <td>7440</td>\n",
       "      <td>25</td>\n",
       "      <td>223081</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>UCDzMNrwWXBWn_bbQ6wk5KiQ</td>\n",
       "      <td>OftenTrain</td>\n",
       "      <td>If you want to become a goat Data Engineer, be...</td>\n",
       "      <td>None</td>\n",
       "      <td>2017-06-09T18:17:43Z</td>\n",
       "      <td>82</td>\n",
       "      <td>26</td>\n",
       "      <td>6961</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>UCFsGwtv75tC3f_GIKQQ5sTA</td>\n",
       "      <td>Nucleus - Data Engineering</td>\n",
       "      <td>Your go-to channel for data engineering projec...</td>\n",
       "      <td>None</td>\n",
       "      <td>2025-04-24T12:31:04.569681Z</td>\n",
       "      <td>704</td>\n",
       "      <td>34</td>\n",
       "      <td>18358</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                  channel_id                   channel_title  \\\n",
       "0   UCoUjs_Z9JhDEi6g0Tba2wJA              Tech Sagar Bhujang   \n",
       "1   UCng7Xx4xhx6pyD1pj1aZQpg                         SFDCGYM   \n",
       "2   UCaTdPhA5f8NC0SZ8uSzsiEw                   Pule‚Äôs diary    \n",
       "3   UCW21hcjn5O7_MLNxIy8kH8g                Tech Career Hubs   \n",
       "4   UCIPDZxzZn-c0_B7OHphL3QQ                    CodeQueryHub   \n",
       "5   UCWmwh8DhJstPi34JzUZFyBw                     DataPopkorn   \n",
       "6   UCYJhto4Of0p8eKKxmB2un9g                 FrontLinesMedia   \n",
       "7   UC_n9wCmDG064tZUKZF2g4Aw                     WafaStudies   \n",
       "8   UCdzWNo6L2TELV-3Gb9dBMWQ                Rasoel Barakhoev   \n",
       "9   UC5e3U5r0gCO_--vi2g-vDvg           Worleybird Innovation   \n",
       "10  UC4-iDkADam__NA5ZPh7YrpQ       Interview Kickstart India   \n",
       "11  UCl8BC-R6fqITW9UrSXj5Uxg                Azarudeen Shahul   \n",
       "12  UCjKtoq0I2vRqk7HFgb07U3g               Culture Data & IA   \n",
       "13  UCucTCSBBt5pCzoBt4P9XsKw           School of Engineering   \n",
       "14  UCbjozK_PYCTLEluFlrJ8UZg        Durga Software Solutions   \n",
       "15  UCkC559DvvBCiLUCH_qHNpCA  Data Engineering with Subhadip   \n",
       "16  UCEmQS-xEmfJ2GY-ckesPa9Q             Geospatial Solution   \n",
       "17  UCUTDd3ieNmKDtHu4VJAsNZA               Data With Pranjal   \n",
       "18  UCDzMNrwWXBWn_bbQ6wk5KiQ                      OftenTrain   \n",
       "19  UCFsGwtv75tC3f_GIKQQ5sTA      Nucleus - Data Engineering   \n",
       "\n",
       "                                          description country  \\\n",
       "0   Hey Guys,\\n\\nWelcome to my YouTube channel!!!\\...      IN   \n",
       "1   SFDCGYM‚Äôs Live Class Training Program helps te...      IN   \n",
       "2   Hey, I‚Äôm Pule, a data engineer by profession. ...    None   \n",
       "3   Hi everyone!\\nMy name is Venkata Sri Hari, and...      IN   \n",
       "4   Welcome to CodeQueryHub ‚Äì your hub for masteri...      IN   \n",
       "5   DataPopkorn.com offers quick, expert-led data ...      GB   \n",
       "6   Frontlines Media is a new generation Edu Tech ...      IN   \n",
       "7   In this Channel, you can find Videos and Playl...      IN   \n",
       "8   My mission is to help you launch a high-paying...      NL   \n",
       "9   Worleybird Innovation - Where AI Meets Actual ...      US   \n",
       "10  Welcome to Interview Kickstart. \\nThe India's ...    None   \n",
       "11  Tutorial on Apache Spark (PySpark), Spark with...      IN   \n",
       "12  Abonne-toi si tu es int√©ress√©(e) par la Data. ...    None   \n",
       "13  Au coeur du nouveau Campus Energypolis de Sion...      CH   \n",
       "14  DURGA Software Solutions is an Institute, whic...      IN   \n",
       "15  This channel is for demonstrating the data pro...      CA   \n",
       "16                                                         KE   \n",
       "17  Welcome to \"Data with Pranjal\"\\nI'm Pranjal, a...      IN   \n",
       "18  If you want to become a goat Data Engineer, be...    None   \n",
       "19  Your go-to channel for data engineering projec...    None   \n",
       "\n",
       "                   published_at  subscriber_count  video_count  view_count  \n",
       "0   2020-05-17T14:44:16.169665Z                17           13        4461  \n",
       "1   2021-08-20T09:56:47.760813Z              1140           33       63814  \n",
       "2   2020-04-29T11:21:42.306197Z                 2            2         232  \n",
       "3   2025-04-02T06:05:11.428129Z                75           14        1922  \n",
       "4   2025-09-26T05:13:27.804913Z                79           34        2649  \n",
       "5   2024-07-22T22:20:41.883101Z               303          176        5895  \n",
       "6          2019-08-05T13:01:42Z            450000         2222    61917694  \n",
       "7          2012-06-02T06:54:56Z            108000          561    16973403  \n",
       "8          2017-11-02T13:51:20Z                 6           12        1458  \n",
       "9   2025-07-23T19:04:10.526848Z                27          106       20673  \n",
       "10  2025-07-17T13:30:11.677909Z              9440           83       85154  \n",
       "11         2012-03-31T15:41:39Z             14500          126     1319831  \n",
       "12  2025-03-21T06:36:52.099792Z               642           21        2009  \n",
       "13  2022-10-24T06:33:05.502043Z               184          119       40314  \n",
       "14         2014-02-03T04:15:47Z            850000        54584   188227461  \n",
       "15         2017-12-09T12:05:51Z                35          120        9769  \n",
       "16  2021-11-17T11:58:50.040058Z              5160           92      224446  \n",
       "17   2024-06-14T17:32:49.45086Z              7440           25      223081  \n",
       "18         2017-06-09T18:17:43Z                82           26        6961  \n",
       "19  2025-04-24T12:31:04.569681Z               704           34       18358  "
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "channel_ids = videos_df['channel_id'].dropna().unique().tolist()\n",
    "channels_df = get_channel_details(channel_ids)\n",
    "channels_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "5d6e67f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_video_statistics(video_ids):\n",
    "    \"\"\"\n",
    "    Retrieve engagement metrics for videos.\n",
    "    Returns DataFrame with video statistics.\n",
    "    \"\"\"\n",
    "    if not video_ids:\n",
    "        return pd.DataFrame()\n",
    "    \n",
    "    try:\n",
    "        youtube = get_youtube_client()\n",
    "        all_stats = []\n",
    "        for i in range(0, len(video_ids), 50):\n",
    "            batch = video_ids[i:i+50]\n",
    "            response = youtube.videos().list(\n",
    "                part=\"statistics,snippet,contentDetails\",\n",
    "                id=\",\".join(batch)\n",
    "            ).execute()\n",
    "\n",
    "            for item in response.get('items', []):\n",
    "                stats = item['statistics']\n",
    "                snippet = item['snippet']\n",
    "                details = item['contentDetails']\n",
    "\n",
    "                all_stats.append({\n",
    "                    'video_id': item['id'],\n",
    "                    'category_id': snippet.get('categoryId'),\n",
    "                    'duration': details.get('duration'),\n",
    "                    'view_count': int(stats.get('viewCount', 0)),\n",
    "                    'like_count': int(stats.get('likeCount', 0)),\n",
    "                    'comment_count': int(stats.get('commentCount', 0)),\n",
    "                    'tags': ','.join(snippet.get('tags', [])) if snippet.get('tags') else None,\n",
    "                    'favorite_count': int(stats.get('favoriteCount', 0)),\n",
    "                    'collected_at': datetime.utcnow().isoformat()\n",
    "                })\n",
    "        \n",
    "        return pd.DataFrame(all_stats)\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"Error in get_video_statistics: {str(e)}\")\n",
    "        return pd.DataFrame()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "0c491513",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adria\\AppData\\Local\\Temp\\ipykernel_32536\\2151632614.py:33: DeprecationWarning: datetime.datetime.utcnow() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.now(datetime.UTC).\n",
      "  'collected_at': datetime.utcnow().isoformat()\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>video_id</th>\n",
       "      <th>category_id</th>\n",
       "      <th>duration</th>\n",
       "      <th>view_count</th>\n",
       "      <th>like_count</th>\n",
       "      <th>comment_count</th>\n",
       "      <th>tags</th>\n",
       "      <th>favorite_count</th>\n",
       "      <th>collected_at</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>F6lad-lTI8A</td>\n",
       "      <td>27</td>\n",
       "      <td>PT43S</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>#databricks data engineer assoicate,#certifica...</td>\n",
       "      <td>0</td>\n",
       "      <td>2025-10-13T15:06:22.087682</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0aepiWWFutw</td>\n",
       "      <td>22</td>\n",
       "      <td>PT39S</td>\n",
       "      <td>142</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>None</td>\n",
       "      <td>0</td>\n",
       "      <td>2025-10-13T15:06:22.087701</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>rLJP85qE8J4</td>\n",
       "      <td>22</td>\n",
       "      <td>PT4M24S</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>None</td>\n",
       "      <td>0</td>\n",
       "      <td>2025-10-13T15:06:22.087708</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>CX0fn0gbb4k</td>\n",
       "      <td>22</td>\n",
       "      <td>PT3M12S</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>None</td>\n",
       "      <td>0</td>\n",
       "      <td>2025-10-13T15:06:22.087714</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>NU2Cu9JHYKM</td>\n",
       "      <td>27</td>\n",
       "      <td>PT22S</td>\n",
       "      <td>1008</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>daily job updates,flm jobs,frontlines media jo...</td>\n",
       "      <td>0</td>\n",
       "      <td>2025-10-13T15:06:22.087721</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      video_id category_id duration  view_count  like_count  comment_count  \\\n",
       "0  F6lad-lTI8A          27    PT43S           1           0              0   \n",
       "1  0aepiWWFutw          22    PT39S         142           4              0   \n",
       "2  rLJP85qE8J4          22  PT4M24S           2           0              0   \n",
       "3  CX0fn0gbb4k          22  PT3M12S           1           0              0   \n",
       "4  NU2Cu9JHYKM          27    PT22S        1008           0              0   \n",
       "\n",
       "                                                tags  favorite_count  \\\n",
       "0  #databricks data engineer assoicate,#certifica...               0   \n",
       "1                                               None               0   \n",
       "2                                               None               0   \n",
       "3                                               None               0   \n",
       "4  daily job updates,flm jobs,frontlines media jo...               0   \n",
       "\n",
       "                 collected_at  \n",
       "0  2025-10-13T15:06:22.087682  \n",
       "1  2025-10-13T15:06:22.087701  \n",
       "2  2025-10-13T15:06:22.087708  \n",
       "3  2025-10-13T15:06:22.087714  \n",
       "4  2025-10-13T15:06:22.087721  "
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "video_ids = videos_df['video_id'].tolist()\n",
    "video_stats_df = get_video_statistics(video_ids)\n",
    "video_stats_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56d4240a",
   "metadata": {},
   "source": [
    "We can only fetch up to 100 comments per video per API call, and the quota cost is 1 per video"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "c9e38d21",
   "metadata": {},
   "outputs": [],
   "source": [
    "from googleapiclient.errors import HttpError\n",
    "\n",
    "def get_video_comments(video_id, max_comments=50):\n",
    "    \"\"\"\n",
    "    Retrieve top-level comments for a video.\n",
    "    Returns DataFrame with comment data, or None if comments are disabled.\n",
    "    \"\"\"\n",
    "    if not video_id:\n",
    "        return None\n",
    "    \n",
    "    try:\n",
    "        youtube = get_youtube_client()\n",
    "        comments = []\n",
    "        next_page_token = None\n",
    "        total_fetched = 0\n",
    "\n",
    "        while total_fetched < max_comments:\n",
    "            response = youtube.commentThreads().list(\n",
    "                part=\"snippet\",\n",
    "                videoId=video_id,\n",
    "                maxResults=min(100, max_comments - total_fetched),\n",
    "                pageToken=next_page_token,\n",
    "                textFormat=\"plainText\"\n",
    "            ).execute()\n",
    "\n",
    "            for item in response.get(\"items\", []):\n",
    "                snippet = item[\"snippet\"][\"topLevelComment\"][\"snippet\"]\n",
    "                comments.append({\n",
    "                    \"video_id\": video_id,\n",
    "                    \"comment_id\": item[\"id\"],\n",
    "                    \"author_display_name\": snippet.get(\"authorDisplayName\"),\n",
    "                    \"text_display\": snippet.get(\"textDisplay\"),\n",
    "                    \"like_count\": snippet.get(\"likeCount\", 0),\n",
    "                    \"published_at\": snippet.get(\"publishedAt\"),\n",
    "                })\n",
    "\n",
    "            total_fetched += len(response.get(\"items\", []))\n",
    "            next_page_token = response.get(\"nextPageToken\")\n",
    "\n",
    "            if not next_page_token:\n",
    "                break\n",
    "        \n",
    "        return pd.DataFrame(comments) if comments else None\n",
    "    \n",
    "    except HttpError as e:\n",
    "        error_json = e.content.decode(\"utf-8\")\n",
    "        if \"commentsDisabled\" in error_json:\n",
    "            print(f\"Comments disabled for video {video_id}\")\n",
    "            return None\n",
    "        else:\n",
    "            print(f\"HttpError fetching comments for {video_id}: {error_json}\")\n",
    "            return None\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Unexpected error fetching comments for {video_id}: {e}\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "282d1444",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No comments found or comments disabled for video F6lad-lTI8A\n",
      "No comments found or comments disabled for video 0aepiWWFutw\n",
      "Comments disabled for video rLJP85qE8J4\n",
      "No comments found or comments disabled for video rLJP85qE8J4\n",
      "No comments found or comments disabled for video CX0fn0gbb4k\n",
      "No comments found or comments disabled for video NU2Cu9JHYKM\n",
      "No comments found or comments disabled for video c_OAaDgP-jM\n",
      "No comments found or comments disabled for video ASUUyeknRoQ\n",
      "No comments found or comments disabled for video p5V4tv7OLQU\n",
      "No comments found or comments disabled for video JFbns0TrshE\n",
      "No comments found or comments disabled for video khweBbtDbEg\n",
      "No comments found or comments disabled for video 73fdeig60QU\n",
      "No comments found or comments disabled for video q8UUqsVRFhc\n",
      "No comments found or comments disabled for video gkgdsEAZfnw\n",
      "No comments found or comments disabled for video 1VvOWOsLS3c\n",
      "No comments found or comments disabled for video 2QQry1pzeRU\n",
      "No comments found or comments disabled for video Ep-kGL_2_Hs\n",
      "No comments found or comments disabled for video eaJ4yR3eX8I\n",
      "Successfully fetched 7 comments across 11 videos.\n"
     ]
    }
   ],
   "source": [
    "all_comments = []\n",
    "\n",
    "if not videos_df.empty:\n",
    "    for i in range(len(videos_df)):\n",
    "        video_id = videos_df[\"video_id\"].iloc[i]\n",
    "        temp_comments = get_video_comments(video_id, max_comments=50)\n",
    "        \n",
    "        if temp_comments is not None and not temp_comments.empty:\n",
    "            all_comments.append(temp_comments)\n",
    "        else:\n",
    "            print(f\"No comments found or comments disabled for video {video_id}\")\n",
    "    \n",
    "    # Combine all comments into a single DataFrame (if any found)\n",
    "    comments_df = pd.concat(all_comments, ignore_index=True) if all_comments else None\n",
    "else:\n",
    "    comments_df = None\n",
    "\n",
    "if comments_df is not None:\n",
    "    print(f\"Successfully fetched {len(comments_df)} comments across {len(all_comments)} videos.\")\n",
    "else:\n",
    "    print(\"No comments available for any of the selected videos.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "02dc806c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>video_id</th>\n",
       "      <th>comment_id</th>\n",
       "      <th>author_display_name</th>\n",
       "      <th>text_display</th>\n",
       "      <th>like_count</th>\n",
       "      <th>published_at</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ZzoX4Gbg4Sk</td>\n",
       "      <td>UgwCj-7r1AYAvPwjSKx4AaABAg</td>\n",
       "      <td>@tarun4494</td>\n",
       "      <td>Your play list are good. I have gone through b...</td>\n",
       "      <td>2</td>\n",
       "      <td>2025-10-13T10:28:36Z</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ZzoX4Gbg4Sk</td>\n",
       "      <td>UgwjuvMd0eD0DMY75p14AaABAg</td>\n",
       "      <td>@mohammadafzal3580</td>\n",
       "      <td>Are you conducting any live sessions on Azure ...</td>\n",
       "      <td>2</td>\n",
       "      <td>2025-10-13T10:14:29Z</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ZzoX4Gbg4Sk</td>\n",
       "      <td>UgyQohs8ex-U5gbx2TJ4AaABAg</td>\n",
       "      <td>@Aravind-gz3gx</td>\n",
       "      <td>Since the adf videos are very old, does anythi...</td>\n",
       "      <td>2</td>\n",
       "      <td>2025-10-12T15:52:24Z</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>M7OlHu-M97Y</td>\n",
       "      <td>UgxzNAPea3b1Mos3Gw14AaABAg</td>\n",
       "      <td>@sivenathimatayi</td>\n",
       "      <td>I also want to go hiking eTable mountain, how ...</td>\n",
       "      <td>0</td>\n",
       "      <td>2025-10-13T07:39:28Z</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>M7OlHu-M97Y</td>\n",
       "      <td>Ugz7rML7REqwWDkCmpF4AaABAg</td>\n",
       "      <td>@sivenathimatayi</td>\n",
       "      <td>First vlogüíÉüèæ can't wait for more!</td>\n",
       "      <td>0</td>\n",
       "      <td>2025-10-13T07:38:27Z</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      video_id                  comment_id author_display_name  \\\n",
       "0  ZzoX4Gbg4Sk  UgwCj-7r1AYAvPwjSKx4AaABAg          @tarun4494   \n",
       "1  ZzoX4Gbg4Sk  UgwjuvMd0eD0DMY75p14AaABAg  @mohammadafzal3580   \n",
       "2  ZzoX4Gbg4Sk  UgyQohs8ex-U5gbx2TJ4AaABAg      @Aravind-gz3gx   \n",
       "3  M7OlHu-M97Y  UgxzNAPea3b1Mos3Gw14AaABAg    @sivenathimatayi   \n",
       "4  M7OlHu-M97Y  Ugz7rML7REqwWDkCmpF4AaABAg    @sivenathimatayi   \n",
       "\n",
       "                                        text_display  like_count  \\\n",
       "0  Your play list are good. I have gone through b...           2   \n",
       "1  Are you conducting any live sessions on Azure ...           2   \n",
       "2  Since the adf videos are very old, does anythi...           2   \n",
       "3  I also want to go hiking eTable mountain, how ...           0   \n",
       "4                  First vlogüíÉüèæ can't wait for more!           0   \n",
       "\n",
       "           published_at  \n",
       "0  2025-10-13T10:28:36Z  \n",
       "1  2025-10-13T10:14:29Z  \n",
       "2  2025-10-12T15:52:24Z  \n",
       "3  2025-10-13T07:39:28Z  \n",
       "4  2025-10-13T07:38:27Z  "
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "comments_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81f7376a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_video_categories(region_code=\"US\"):\n",
    "    \"\"\"\n",
    "    Retrieve video categories for a specific region.\n",
    "    Returns DataFrame mapping category_id to category_name.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        youtube = get_youtube_client()\n",
    "        response = youtube.videoCategories().list(\n",
    "            part=\"snippet\",\n",
    "            regionCode=region_code\n",
    "        ).execute()\n",
    "\n",
    "        categories = []\n",
    "        for item in response.get(\"items\", []):\n",
    "            snippet = item[\"snippet\"]\n",
    "            categories.append({\n",
    "                \"category_id\": item[\"id\"],\n",
    "                \"category_title\": snippet[\"title\"],\n",
    "                \"assignable\": snippet[\"assignable\"],\n",
    "                \"region\": region_code\n",
    "            })\n",
    "        \n",
    "        return pd.DataFrame(categories)\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"Error in get_video_categories: {str(e)}\")\n",
    "        return pd.DataFrame()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "6680de42",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>category_id</th>\n",
       "      <th>title</th>\n",
       "      <th>assignable</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>Film &amp; Animation</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>Autos &amp; Vehicles</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>10</td>\n",
       "      <td>Music</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>15</td>\n",
       "      <td>Pets &amp; Animals</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>17</td>\n",
       "      <td>Sports</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  category_id             title  assignable\n",
       "0           1  Film & Animation        True\n",
       "1           2  Autos & Vehicles        True\n",
       "2          10             Music        True\n",
       "3          15    Pets & Animals        True\n",
       "4          17            Sports        True"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "categories_df = get_video_categories(region_code=\"US\")\n",
    "categories_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50c82c90",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optional: Get all uploaded videos from a channel using the 'Uploads' playlist\n",
    "\n",
    "def get_channel_uploads(channel_id: str, max_results: int = 50):\n",
    "    \"\"\"\n",
    "    Retrieves all uploaded videos from a given YouTube channel using the 'Uploads' playlist.\n",
    "\n",
    "    Parameters:\n",
    "        channel_id (str): The YouTube channel ID.\n",
    "        max_results (int): Max number of results per API call (default=50, API limit).\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: DataFrame containing all uploaded videos' metadata.\n",
    "    \"\"\"\n",
    "    # Get the uploads playlist ID for the channel\n",
    "    channel_response = youtube.channels().list(\n",
    "        part=\"contentDetails\",\n",
    "        id=channel_id\n",
    "    ).execute()\n",
    "\n",
    "    uploads_playlist_id = (\n",
    "        channel_response[\"items\"][0][\"contentDetails\"][\"relatedPlaylists\"][\"uploads\"]\n",
    "    )\n",
    "\n",
    "    # Retrieve all videos from that playlist\n",
    "    videos = []\n",
    "    next_page_token = None\n",
    "\n",
    "    while True:\n",
    "        request = youtube.playlistItems().list(\n",
    "            part=\"snippet,contentDetails\",\n",
    "            playlistId=uploads_playlist_id,\n",
    "            maxResults=max_results,\n",
    "            pageToken=next_page_token\n",
    "        )\n",
    "        response = request.execute()\n",
    "\n",
    "        for item in response[\"items\"]:\n",
    "            snippet = item[\"snippet\"]\n",
    "            content = item[\"contentDetails\"]\n",
    "            videos.append({\n",
    "                \"playlist_item_id\": item[\"id\"],\n",
    "                \"playlist_id\": snippet.get(\"playlistId\"),\n",
    "                \"video_id\": content.get(\"videoId\"),\n",
    "                \"channel_id\": snippet.get(\"channelId\"),\n",
    "                \"title\": snippet.get(\"title\"),\n",
    "                \"description\": snippet.get(\"description\"),\n",
    "                \"published_at\": snippet.get(\"publishedAt\"),\n",
    "                \"video_published_at\": content.get(\"videoPublishedAt\"),\n",
    "                \"position\": snippet.get(\"position\")\n",
    "            })\n",
    "\n",
    "        next_page_token = response.get(\"nextPageToken\")\n",
    "        if not next_page_token:\n",
    "            break\n",
    "\n",
    "    df = pd.DataFrame(videos)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4e6baad3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>playlist_item_id</th>\n",
       "      <th>playlist_id</th>\n",
       "      <th>video_id</th>\n",
       "      <th>channel_id</th>\n",
       "      <th>title</th>\n",
       "      <th>description</th>\n",
       "      <th>published_at</th>\n",
       "      <th>video_published_at</th>\n",
       "      <th>position</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>VVV0SzRRQWN6QU4ybXQyb3dfamxHaW5RLkRjamFwSUViYy1z</td>\n",
       "      <td>UUtK4QAczAN2mt2ow_jlGinQ</td>\n",
       "      <td>DcjapIEbc-s</td>\n",
       "      <td>UCtK4QAczAN2mt2ow_jlGinQ</td>\n",
       "      <td>Every LAST-MINUTE winner! | Rooney, Grealish, ...</td>\n",
       "      <td>Jack Grealish scored Everton's 27th 'last-minu...</td>\n",
       "      <td>2025-10-08T15:20:56Z</td>\n",
       "      <td>2025-10-08T15:20:56Z</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>VVV0SzRRQWN6QU4ybXQyb3dfamxHaW5RLkRZdUJEYnhJS2g4</td>\n",
       "      <td>UUtK4QAczAN2mt2ow_jlGinQ</td>\n",
       "      <td>DYuBDbxIKh8</td>\n",
       "      <td>UCtK4QAczAN2mt2ow_jlGinQ</td>\n",
       "      <td>James Tarkowski signs Everton contract extensi...</td>\n",
       "      <td>James Tarkowski has signed a two-year contract...</td>\n",
       "      <td>2025-10-08T11:00:31Z</td>\n",
       "      <td>2025-10-08T11:00:31Z</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>VVV0SzRRQWN6QU4ybXQyb3dfamxHaW5RLmRUSzA1bUtxYlRV</td>\n",
       "      <td>UUtK4QAczAN2mt2ow_jlGinQ</td>\n",
       "      <td>dTK05mKqbTU</td>\n",
       "      <td>UCtK4QAczAN2mt2ow_jlGinQ</td>\n",
       "      <td>They said it would never get built... üíô Hill D...</td>\n",
       "      <td>Subscribe to Everton Football Club's official ...</td>\n",
       "      <td>2025-10-08T09:44:51Z</td>\n",
       "      <td>2025-10-08T09:44:51Z</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>VVV0SzRRQWN6QU4ybXQyb3dfamxHaW5RLmR3d2Y3YnRjVUk0</td>\n",
       "      <td>UUtK4QAczAN2mt2ow_jlGinQ</td>\n",
       "      <td>dwwf7btcUI4</td>\n",
       "      <td>UCtK4QAczAN2mt2ow_jlGinQ</td>\n",
       "      <td>Grealish winner ends Palace run! | Extended hi...</td>\n",
       "      <td>Jack Grealish's stoppage-time winner earned Ev...</td>\n",
       "      <td>2025-10-06T23:00:42Z</td>\n",
       "      <td>2025-10-06T23:00:42Z</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>VVV0SzRRQWN6QU4ybXQyb3dfamxHaW5RLm5IdF9vVUdsb0xv</td>\n",
       "      <td>UUtK4QAczAN2mt2ow_jlGinQ</td>\n",
       "      <td>nHt_oUGloLo</td>\n",
       "      <td>UCtK4QAczAN2mt2ow_jlGinQ</td>\n",
       "      <td>GREALISH'S LAST-GASP WINNER FROM PITCHSIDE + B...</td>\n",
       "      <td>The latest episode of In HD ‚Äì our new behind-t...</td>\n",
       "      <td>2025-10-06T14:32:21Z</td>\n",
       "      <td>2025-10-06T14:32:21Z</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                   playlist_item_id               playlist_id  \\\n",
       "0  VVV0SzRRQWN6QU4ybXQyb3dfamxHaW5RLkRjamFwSUViYy1z  UUtK4QAczAN2mt2ow_jlGinQ   \n",
       "1  VVV0SzRRQWN6QU4ybXQyb3dfamxHaW5RLkRZdUJEYnhJS2g4  UUtK4QAczAN2mt2ow_jlGinQ   \n",
       "2  VVV0SzRRQWN6QU4ybXQyb3dfamxHaW5RLmRUSzA1bUtxYlRV  UUtK4QAczAN2mt2ow_jlGinQ   \n",
       "3  VVV0SzRRQWN6QU4ybXQyb3dfamxHaW5RLmR3d2Y3YnRjVUk0  UUtK4QAczAN2mt2ow_jlGinQ   \n",
       "4  VVV0SzRRQWN6QU4ybXQyb3dfamxHaW5RLm5IdF9vVUdsb0xv  UUtK4QAczAN2mt2ow_jlGinQ   \n",
       "\n",
       "      video_id                channel_id  \\\n",
       "0  DcjapIEbc-s  UCtK4QAczAN2mt2ow_jlGinQ   \n",
       "1  DYuBDbxIKh8  UCtK4QAczAN2mt2ow_jlGinQ   \n",
       "2  dTK05mKqbTU  UCtK4QAczAN2mt2ow_jlGinQ   \n",
       "3  dwwf7btcUI4  UCtK4QAczAN2mt2ow_jlGinQ   \n",
       "4  nHt_oUGloLo  UCtK4QAczAN2mt2ow_jlGinQ   \n",
       "\n",
       "                                               title  \\\n",
       "0  Every LAST-MINUTE winner! | Rooney, Grealish, ...   \n",
       "1  James Tarkowski signs Everton contract extensi...   \n",
       "2  They said it would never get built... üíô Hill D...   \n",
       "3  Grealish winner ends Palace run! | Extended hi...   \n",
       "4  GREALISH'S LAST-GASP WINNER FROM PITCHSIDE + B...   \n",
       "\n",
       "                                         description          published_at  \\\n",
       "0  Jack Grealish scored Everton's 27th 'last-minu...  2025-10-08T15:20:56Z   \n",
       "1  James Tarkowski has signed a two-year contract...  2025-10-08T11:00:31Z   \n",
       "2  Subscribe to Everton Football Club's official ...  2025-10-08T09:44:51Z   \n",
       "3  Jack Grealish's stoppage-time winner earned Ev...  2025-10-06T23:00:42Z   \n",
       "4  The latest episode of In HD ‚Äì our new behind-t...  2025-10-06T14:32:21Z   \n",
       "\n",
       "     video_published_at  position  \n",
       "0  2025-10-08T15:20:56Z         0  \n",
       "1  2025-10-08T11:00:31Z         1  \n",
       "2  2025-10-08T09:44:51Z         2  \n",
       "3  2025-10-06T23:00:42Z         3  \n",
       "4  2025-10-06T14:32:21Z         4  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "channel_id = \"UCtK4QAczAN2mt2ow_jlGinQ\"\n",
    "\n",
    "df_uploads = get_channel_uploads(channel_id)\n",
    "df_uploads.head()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
